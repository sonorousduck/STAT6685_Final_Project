{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(self, df, transformation, target_sample_rate, duration):\n",
    "        self.audio_paths = df['filename'].values\n",
    "        self.labels = df['primary_label_encoded'].values\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = target_sample_rate * duration\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = f\"data/{self.audio_paths[index]}\"\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Check if our sample rate is the same as the target sameple rate. If not, resample\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        \n",
    "        # Check shape and verify it is correct\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, axis=0, keepdim=True)\n",
    "        \n",
    "        # Check the number of samples and pad/truncate as needed\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        \n",
    "        elif signal.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = F.pad(signal, last_dim_padding)\n",
    "        \n",
    "        # Then we can do signal processing. This tutorial uses the Mel Spectrogram, so I will leave that in for right now. This may not be what we want to go with in the end\n",
    "        mel = self.transformation(signal)\n",
    "\n",
    "        # Transforms mel into a 3 channel image (This is for RESNET)\n",
    "        image = torch.cat([mel, mel, mel])\n",
    "\n",
    "        # Normalize the image\n",
    "        max_val = torch.abs(image).max()\n",
    "        image = image / max_val\n",
    "\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_metadata.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['primary_label_encoded'] = encoder.fit_transform(df['primary_label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(df, df['primary_label_encoded'], test_size= .2, random_state=7)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 32_000\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 64\n",
    "num_classes = 152\n",
    "duration = 7\n",
    "n_mels = 64\n",
    "\n",
    "def get_data():\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sr,\n",
    "                                                            n_fft=n_fft,\n",
    "                                                            hop_length=hop_length,\n",
    "                                                            n_mels=64)\n",
    "\n",
    "    train_dataset = BirdClefDataset(X_train, mel_spectrogram, sr, duration)\n",
    "    valid_dataset = BirdClefDataset(X_val, mel_spectrogram, sr, duration)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, valid_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN Model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(224256, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "train_loader, valid_loader = get_data()\n",
    "\n",
    "# Train Loop\n",
    "load = True\n",
    "model = SimpleModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 1000\n",
    "\n",
    "if load:\n",
    "    model.load_state_dict(torch.load('./model.bin'))\n",
    "\n",
    "best_f1 = 0.0076585670200064\n",
    "\n",
    "loop = tqdm(train_loader, position=0)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loop):\n",
    "        y = y.type(torch.LongTensor)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        _, predicitions = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Run validation loop\n",
    "    if epoch + 1 % 1 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        loop_validation = tqdm(valid_loader, position=0)\n",
    "        pred = []\n",
    "        label = []\n",
    "\n",
    "        for i, (X, y) in enumerate(loop):\n",
    "            y = y.type(torch.LongTensor)\n",
    "            y = y.to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            pred.extend(predictions.view(-1).cpu().detach().numpy())\n",
    "            label.extend(y.view(-1).cpu().detach().numpy())\n",
    "\n",
    "            loop.set_description(f\"Validation Epoch [{epoch + 1}/{epochs}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    valid_f1 = f1_score(label, pred, average='macro')\n",
    "\n",
    "    if valid_f1 > best_f1:\n",
    "        print(f\"Validation F1 Improved - {best_f1} ---> {valid_f1}\")\n",
    "        best_f1 = valid_f1\n",
    "        torch.save(model.state_dict(), f'./model.bin')\n",
    "        print(f\"Saved model checkpoint at ./model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddb17cf83b4972283817f5ab1d22df032d6c30bac9464ed7ca8172a4ffa55b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
