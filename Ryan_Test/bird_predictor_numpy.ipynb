{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(self, df, target_sample_rate, duration):\n",
    "        self.audio_paths = df['filename'].values\n",
    "        self.labels = df['primary_label_encoded'].values\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = target_sample_rate * duration\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = f\"data/{self.audio_paths[index]}\"\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Check if our sample rate is the same as the target sameple rate. If not, resample\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        \n",
    "        # Check shape and verify it is correct\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, axis=0, keepdim=True)\n",
    "        \n",
    "        # Check the number of samples and pad/truncate as needed\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        \n",
    "        elif signal.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = F.pad(signal, last_dim_padding)\n",
    "        \n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return signal, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>primary_label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \\\n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n",
       "\n",
       "   primary_label_encoded  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_metadata.csv')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['primary_label_encoded'] = encoder.fit_transform(df['primary_label'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(df, df['primary_label_encoded'], test_size= .2, random_state=7)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=7)\n",
    "\n",
    "sr = 32_000\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "train_batch_size = 256\n",
    "valid_batch_size = 256\n",
    "num_classes = 152\n",
    "duration = 7\n",
    "n_mels = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    train_dataset = BirdClefDataset(X_train, sr, duration)\n",
    "    valid_dataset = BirdClefDataset(X_val, sr, duration)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, valid_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(224000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_data()\n",
    "\n",
    "# Train Loop\n",
    "load = True\n",
    "model = SimpleModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 150\n",
    "\n",
    "if load:\n",
    "    model.load_state_dict(torch.load('./model_saved_weights/model_numpy.bin'))\n",
    "\n",
    "# This has overtrained a ton. We are at .597 loss on training and 6 on validation\n",
    "best_f1 = 0.011680021168441034\n",
    "total_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/150]: 100%|██████████| 38/38 [07:53<00:00, 12.47s/it, loss=5.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0 ---> 0.0002561910491912559\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/150]: 100%|██████████| 38/38 [07:31<00:00, 11.88s/it, loss=4.6] \n",
      "Validation Epoch [2/150: 100%|██████████| 10/10 [02:05<00:00, 12.54s/it, loss=4.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.0002561910491912559 ---> 0.0026632090593132923\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/150]: 100%|██████████| 38/38 [07:50<00:00, 12.39s/it, loss=4.29]\n",
      "Epoch [4/150]: 100%|██████████| 38/38 [07:28<00:00, 11.80s/it, loss=3.83]\n",
      "Validation Epoch [4/150: 100%|██████████| 10/10 [01:52<00:00, 11.25s/it, loss=4.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.0026632090593132923 ---> 0.003987108425821112\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/150]: 100%|██████████| 38/38 [07:31<00:00, 11.88s/it, loss=3.14]\n",
      "Epoch [6/150]: 100%|██████████| 38/38 [07:26<00:00, 11.75s/it, loss=3.15]\n",
      "Validation Epoch [6/150: 100%|██████████| 10/10 [01:53<00:00, 11.34s/it, loss=4.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.003987108425821112 ---> 0.007702160531071673\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/150]: 100%|██████████| 38/38 [08:10<00:00, 12.91s/it, loss=2.83]\n",
      "Epoch [8/150]: 100%|██████████| 38/38 [07:20<00:00, 11.59s/it, loss=2.64]\n",
      "Validation Epoch [8/150: 100%|██████████| 10/10 [02:09<00:00, 12.95s/it, loss=4.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.007702160531071673 ---> 0.00955722538549094\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/150]: 100%|██████████| 38/38 [07:54<00:00, 12.49s/it, loss=2.8] \n",
      "Epoch [10/150]: 100%|██████████| 38/38 [07:38<00:00, 12.06s/it, loss=1.79]\n",
      "Validation Epoch [10/150: 100%|██████████| 10/10 [01:52<00:00, 11.23s/it, loss=4.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.00955722538549094 ---> 0.010020675971215073\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/150]: 100%|██████████| 38/38 [07:23<00:00, 11.66s/it, loss=1.32]\n",
      "Epoch [12/150]: 100%|██████████| 38/38 [07:19<00:00, 11.57s/it, loss=1.18]\n",
      "Validation Epoch [12/150: 100%|██████████| 10/10 [01:51<00:00, 11.14s/it, loss=4.68]\n",
      "Epoch [13/150]: 100%|██████████| 38/38 [07:17<00:00, 11.51s/it, loss=1.15]\n",
      "Epoch [14/150]: 100%|██████████| 38/38 [07:19<00:00, 11.55s/it, loss=1.24]\n",
      "Validation Epoch [14/150: 100%|██████████| 10/10 [01:50<00:00, 11.05s/it, loss=4.94]\n",
      "Epoch [15/150]: 100%|██████████| 38/38 [07:15<00:00, 11.47s/it, loss=1.09] \n",
      "Epoch [16/150]: 100%|██████████| 38/38 [07:16<00:00, 11.49s/it, loss=0.945]\n",
      "Validation Epoch [16/150: 100%|██████████| 10/10 [01:58<00:00, 11.83s/it, loss=5.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.010020675971215073 ---> 0.010135766329597076\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/150]: 100%|██████████| 38/38 [07:57<00:00, 12.57s/it, loss=0.778]\n",
      "Epoch [18/150]: 100%|██████████| 38/38 [07:51<00:00, 12.41s/it, loss=0.58] \n",
      "Validation Epoch [18/150: 100%|██████████| 10/10 [02:03<00:00, 12.34s/it, loss=7.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Improved - 0.010135766329597076 ---> 0.011680021168441034\n",
      "Saved model checkpoint at ./model_numpy.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/150]: 100%|██████████| 38/38 [07:50<00:00, 12.39s/it, loss=0.367]\n",
      "Epoch [20/150]: 100%|██████████| 38/38 [07:49<00:00, 12.36s/it, loss=0.825]\n",
      "Validation Epoch [20/150: 100%|██████████| 10/10 [01:59<00:00, 11.91s/it, loss=5.59]\n",
      "Epoch [21/150]: 100%|██████████| 38/38 [07:49<00:00, 12.35s/it, loss=1.78] \n",
      "Epoch [22/150]: 100%|██████████| 38/38 [07:49<00:00, 12.35s/it, loss=0.525]\n",
      "Validation Epoch [22/150: 100%|██████████| 10/10 [01:58<00:00, 11.86s/it, loss=5.75]\n",
      "Epoch [23/150]: 100%|██████████| 38/38 [07:36<00:00, 12.00s/it, loss=0.829]\n",
      "Epoch [24/150]: 100%|██████████| 38/38 [07:17<00:00, 11.51s/it, loss=0.475]\n",
      "Validation Epoch [24/150: 100%|██████████| 10/10 [01:50<00:00, 11.03s/it, loss=6.01]\n",
      "Epoch [25/150]: 100%|██████████| 38/38 [07:17<00:00, 11.53s/it, loss=0.167]\n",
      "Epoch [26/150]: 100%|██████████| 38/38 [07:17<00:00, 11.51s/it, loss=0.482]\n",
      "Validation Epoch [26/150: 100%|██████████| 10/10 [01:50<00:00, 11.04s/it, loss=5.69]\n",
      "Epoch [27/150]: 100%|██████████| 38/38 [07:17<00:00, 11.51s/it, loss=0.501]\n",
      "Epoch [28/150]: 100%|██████████| 38/38 [07:17<00:00, 11.51s/it, loss=0.202]\n",
      "Validation Epoch [28/150: 100%|██████████| 10/10 [01:50<00:00, 11.03s/it, loss=5.91]\n",
      "Epoch [29/150]:  42%|████▏     | 16/38 [03:22<04:38, 12.64s/it, loss=0.447]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [63], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m loop \u001b[39m=\u001b[39m tqdm(train_loader, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loop):\n\u001b[0;32m      5\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\n\u001b[0;32m      6\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sonor\\miniconda3\\envs\\pytorch38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [16], line 31\u001b[0m, in \u001b[0;36mBirdClefDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     29\u001b[0m     num_missing_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m-\u001b[39m signal\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m     last_dim_padding \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, num_missing_samples)\n\u001b[1;32m---> 31\u001b[0m     signal \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mpad(signal, last_dim_padding)\n\u001b[0;32m     33\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index])\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m signal, label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, position=0)\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loop):\n",
    "        y = y.type(torch.LongTensor)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Run validation loop\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        loop_validation = tqdm(valid_loader, position=0)\n",
    "        pred = []\n",
    "        label = []\n",
    "\n",
    "        for i, (X, y) in enumerate(loop_validation):\n",
    "            y = y.type(torch.LongTensor)\n",
    "            y = y.to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            pred.extend(predictions.view(-1).cpu().detach().numpy())\n",
    "            label.extend(y.view(-1).cpu().detach().numpy())\n",
    "\n",
    "            loop_validation.set_description(f\"Validation Epoch [{epoch + 1}/{epochs}\")\n",
    "            loop_validation.set_postfix(loss=loss.item())\n",
    "\n",
    "    valid_f1 = f1_score(label, pred, average='macro')\n",
    "    total_f1.append(valid_f1)\n",
    "\n",
    "    with open('f1_score_numpy.txt', 'a') as f:\n",
    "        f.write(f\"{valid_f1}\\n\")\n",
    "\n",
    "    if valid_f1 > best_f1:\n",
    "        print(f\"Validation F1 Improved - {best_f1} ---> {valid_f1}\")\n",
    "        best_f1 = valid_f1\n",
    "        torch.save(model.state_dict(), f'./model_saved_weights/model_numpy.bin')\n",
    "        print(f\"Saved model checkpoint at ./model_numpy.bin\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddb17cf83b4972283817f5ab1d22df032d6c30bac9464ed7ca8172a4ffa55b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
